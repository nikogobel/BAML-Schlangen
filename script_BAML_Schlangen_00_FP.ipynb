{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felix Puetterich\\AppData\\Local\\Temp\\ipykernel_10956\\2575529016.py:31: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_reviews = pd.read_csv(file_path_reviews)\n",
      "C:\\Users\\Felix Puetterich\\AppData\\Local\\Temp\\ipykernel_10956\\2575529016.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_trainset_reviews[\"Like\"] = df_trainset_reviews[\"Like\"].astype(int)\n",
      "C:\\Users\\Felix Puetterich\\AppData\\Local\\Temp\\ipykernel_10956\\2575529016.py:140: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_testset_reviews[\"Like\"] = df_testset_reviews[\"Like\"].fillna(0)\n",
      "C:\\Users\\Felix Puetterich\\AppData\\Local\\Temp\\ipykernel_10956\\2575529016.py:140: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_testset_reviews[\"Like\"] = df_testset_reviews[\"Like\"].fillna(0)\n",
      "C:\\Users\\Felix Puetterich\\AppData\\Local\\Temp\\ipykernel_10956\\2575529016.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_testset_reviews[\"Like\"] = df_testset_reviews[\"Like\"].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Parameter: {'model__max_depth': 4, 'model__max_features': 3, 'model__n_estimators': 50} (CV Score=0.819)\n",
      "0.5\n",
      "true      0     1\n",
      "pred             \n",
      "1     13150  1734\n",
      "Score on test set: 0.6541695282452055\n",
      "true      0    1\n",
      "pred            \n",
      "0     10842  895\n",
      "1      2308  839\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "import types\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from fractions import Fraction\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "\n",
    "# load the data\n",
    "file_path_diet = \"diet.csv\"\n",
    "file_path_reviews = \"reviews.csv\"\n",
    "file_path_requests = \"requests.csv\"\n",
    "file_path_recipes = \"recipes.csv\"\n",
    "\n",
    "df_diet = pd.read_csv(file_path_diet)\n",
    "df_reviews = pd.read_csv(file_path_reviews)\n",
    "df_requests = pd.read_csv(file_path_requests)\n",
    "df_recipes = pd.read_csv(file_path_recipes)\n",
    "df_testset_reviews = None\n",
    "df_trainset_reviews = None\n",
    "df_diet_clean = None\n",
    "df_recipes_clean = None\n",
    "df_requests_clean = None\n",
    "df_trainset_clean =None\n",
    "df_testset_clean =None\n",
    "\n",
    "#-------------- Hilfsfunktionen Data Cleaning----------------------------------\n",
    "def erstelle_Meal_Typen():\n",
    "    global df_recipes\n",
    "    #Bringe Zutaten in Listenform\n",
    "    df_recipes[\"RecipeIngredientParts\"]= df_recipes[\"RecipeIngredientParts\"].apply(extract_list_elements)\n",
    "    \n",
    "    #Kategorisiere Rezepte\n",
    "    df_recipes['Type_Meal'] = df_recipes['RecipeIngredientParts'].apply(categorize_recipe)\n",
    "\n",
    "def extract_list_elements(s):\n",
    "    cleaned_string = s.replace('\\\\\"', '')\n",
    "    cleaned_string = cleaned_string.replace('c(', '[')\n",
    "    cleaned_string = cleaned_string.replace(')', ']')\n",
    "    if s.find(\"c(\") != -1:\n",
    "        return json.loads(cleaned_string)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def categorize_recipe(ingredients):\n",
    "    #Definiere Omnivore und Vegetarische Keywords sowie Exceptions\n",
    "    omnivor_keywords = set([\"chicken\", \"beef\", \"pork\", \"lamb\",\"turkey\", \"duck\", \"goat\",\"bacon\", \"ham\",\n",
    "                            \"meat\",\"sausage\", \"salami\", \"hot dog\",\"salmon\", \"tuna\", \"trout\", \"lobster\", \n",
    "                            \"cod\", \"shrimp\",\"crab\", \"oyster\",\"squid\", \"octopus\", \"liver\", \"heart\", \"tongue\", \n",
    "                            \"steak\", \"sea bass\", \"clams\", \"halibuts\", \"fillet\", \"scallops\", \"fish\",]) \n",
    "    vegetarian_keywords = set([\"milk\", \"cheese\", \"butter\", \"yogurt\", \"cream\", \"egg\", \"honey\", \"gelatin\", \n",
    "                               \"whey\", \"casein\", \"lactose\", \"dairy\", \"mayonnaise\", \"mozzarella\"])\n",
    "    vegetarian_exceptions = set([\"soymilk\", \"almond butter\", \"peanut butter\", \"vegan butter\",\"coconut milk\", \n",
    "                                 \"coconut butter\", \"cashew butter\"])\n",
    "    #Durchsuche und Klassifiziere jedes Rezept\n",
    "    for ingredient in ingredients:\n",
    "        if any(omnivore_keyword in ingredient for omnivore_keyword in omnivor_keywords):\n",
    "            return 'Omnivor'\n",
    "        elif any(vegetarian_keyword in ingredient for vegetarian_keyword in vegetarian_keywords):\n",
    "            if not any(exception in ingredient for exception in vegetarian_exceptions):\n",
    "                return 'Vegetarisch'\n",
    "    return 'Vegan'\n",
    "\n",
    "def translate_recipe_yields_to_categories(s):\n",
    "    if s == \"Undefined\":\n",
    "        return \"Undefined\"\n",
    "    result_list = s.split()\n",
    "    for value in result_list[:1]:\n",
    "        # print(value)\n",
    "        if value.find(\"-\") != -1:\n",
    "            value = s.split(\"-\")[0]\n",
    "            # print(\"update\")\n",
    "            # print(value)\n",
    "        fraction_obj = Fraction(value)\n",
    "        val_int: float = float(fraction_obj)\n",
    "        if val_int == 1:\n",
    "            return \"Single Portion\"\n",
    "        elif val_int == 2:\n",
    "            return \"Two Portions\"\n",
    "        elif val_int <= 5:\n",
    "            return \"Medium Portions\"\n",
    "        elif val_int > 5:\n",
    "            return \"Many Portions\"\n",
    "        else:\n",
    "            return \"error\"\n",
    "\n",
    "\n",
    "\n",
    "#-------------- Cleanen Datasets -------------------------#\n",
    "\n",
    "\n",
    "#Clean Diet.csv\n",
    "# --> Es gibt 271.907 rows und keine fehlenden Werte (ausser bei Diet 1)\n",
    "# --> Es gibt 78.626 Omnivore, 49.897 Vegan, 143.383 Vegetarian User\n",
    "# --> Es gibt keine Outlier bei Age, Diet-Typ ist gleichverteilt ueber Age\n",
    "def clean_diet():\n",
    "    global df_diet\n",
    "    global df_diet_clean\n",
    "    df_diet[\"AuthorId\"] = df_diet[\"AuthorId\"].astype(\"category\")\n",
    "    df_diet[\"Diet\"] = df_diet[\"Diet\"].astype(\"category\")\n",
    "    NAValues_diet = df_diet[\"Diet\"].isna()\n",
    "    df_diet = df_diet.loc[~NAValues_diet]\n",
    "    df_diet_clean =df_diet\n",
    "\n",
    "#1. Clean Reviews.csv \n",
    "# --> Es gibt 140.195 rows, Rating, Like und TestSetID haben fehlende Werte\n",
    "# --> Immer wenn Rows im TestSet sind, haben sie keinen Like Wert: 97381 + 42814 = 140195\n",
    "# --> Rating ist immer 2.0 ansonsten nicht vorhanden\n",
    "#2. Erstelle Test und Training Set\n",
    "def clean_reviews():\n",
    "    global df_reviews\n",
    "    global df_testset_clean\n",
    "    global df_trainset_clean\n",
    "    #Modify Reviews.csv\n",
    "    df_reviews[\"AuthorId\"] = df_reviews[\"AuthorId\"].astype(\"category\")\n",
    "    df_reviews = df_reviews.drop(columns={\"Rating\"})\n",
    "    #Create Test und Trainset\n",
    "    NAValues_Likes = df_reviews[\"Like\"].isna()\n",
    "    df_testset_reviews = df_reviews[NAValues_Likes]\n",
    "    df_trainset_reviews = df_reviews[~NAValues_Likes]\n",
    "    #Modify Trainset\n",
    "    df_trainset_reviews[\"Like\"] = df_trainset_reviews[\"Like\"].astype(int)\n",
    "    df_trainset_reviews = df_trainset_reviews.drop(columns={\"TestSetId\"})\n",
    "    #Modify Testset\n",
    "    df_testset_reviews[\"Like\"] = df_testset_reviews[\"Like\"].fillna(0)\n",
    "    df_testset_reviews[\"Like\"] = df_testset_reviews[\"Like\"].astype(int)\n",
    "    #Save Testset und Trainset\n",
    "    df_trainset_clean = df_testset_reviews\n",
    "    df_trainset_clean = df_trainset_reviews\n",
    "    df_testset_reviews.to_csv(\"cleaned_testset.csv\", index=False)\n",
    "    df_trainset_reviews.to_csv(\"cleaned_trainset.csv\", index=False)\n",
    "\n",
    "#Clean Requests.csv\n",
    "# --> Es gibt 140.195 rows, keine Spalte hat fehlende Werte\n",
    "# --> Time-Attribut hat grosse Outlier\n",
    "# --> Verteilung der Flags: \n",
    "    #High Calories: 0.0 = 83.806, 1.0 = 56.389; \n",
    "    #High Protein: Indifferent = 84.244, Yes = 55.951\n",
    "    #High Fiber: 0 = 83.956, 1 = 56.239\n",
    "    #Low Sugar: 0 = 98.113, Indifferent = 42.082\n",
    "    #Low Fat: 0 = 98.209, 1 = 41.986\n",
    "def clean_requests():\n",
    "    global df_requests\n",
    "    global df_requests_clean\n",
    "    #Kategorisierung der Flags\n",
    "    df_requests[\"HighCalories\"] = df_requests[\"HighCalories\"].astype(\"category\")\n",
    "    df_requests[\"HighProtein\"] = df_requests[\"HighProtein\"].astype(\"category\")\n",
    "    df_requests[\"LowFat\"] = df_requests[\"LowFat\"].astype(\"category\")\n",
    "    df_requests[\"LowSugar\"] = df_requests[\"LowSugar\"].astype(\"category\")\n",
    "    df_requests[\"HighFiber\"] = df_requests[\"HighFiber\"].astype(\"category\")\n",
    "    df_requests[\"AuthorId\"] = df_requests[\"AuthorId\"].astype(\"category\")\n",
    "    #Entfernen von Negativen Time-Werten\n",
    "    df_requests = df_requests[df_requests[\"Time\"]>0]\n",
    "    #Entfernen von Outliern bei Time-Attribut\n",
    "    Q1 = df_requests[\"Time\"].quantile(0.25)\n",
    "    Q3 = df_requests[\"Time\"].quantile(0.75)\n",
    "    IQR = Q3-Q1\n",
    "    df_requests = df_requests[~((df_requests[\"Time\"]>(Q3+3*IQR)))]\n",
    "    #Save Request-Set\n",
    "    df_requests_clean = df_requests\n",
    "    df_requests.to_csv(\"cleaned_requests.csv\", index=False)\n",
    "    \n",
    "\n",
    "#Clean Recipes\n",
    "# --> Es gibt 75604 rows, RecipeServings und RecipeYields haben fehlende Werte\n",
    "# --> CookTime und PrepTime haben grosse Outlier\n",
    "# --> Fast jede XContent Spalte hat grosse Outlier, insbesondere Sodium, Cholesterol, Calories, Fat und Protein\n",
    "# --> RecipeCategory Attribut ist wenig hilfreich, da extrem viele Werte den Wert 'Other' haben. Konkret:\n",
    "    #Beverages         2.303\n",
    "    #Bread             4.246\n",
    "    #Breakfast         3.033\n",
    "    #Lunch             4.887\n",
    "    #One dish meal     4.590\n",
    "    #Other            56.347\n",
    "    #Soup               .198\n",
    "# --> Hohe Korrelationen unter Content Werten\n",
    "def clean_recipes():\n",
    "    global df_recipes\n",
    "    global df_recipes_clean\n",
    "    #1. Entfernen Outliers CookTime\n",
    "    \n",
    "    Q1_CookTime = df_recipes[\"CookTime\"].quantile(0.25)\n",
    "    Q3_CookTime = df_recipes[\"CookTime\"].quantile(0.75)\n",
    "    IQR = Q3_CookTime-Q1_CookTime\n",
    "    df_recipes = df_recipes[~((df_recipes[\"CookTime\"]>((Q3_CookTime+3*IQR))))]\n",
    "    \n",
    "    #2. Entfernen Outliers PrepTime\n",
    "    Q1_PrepTime = df_recipes[\"PrepTime\"].quantile(0.25)\n",
    "    Q3_PrepTime = df_recipes[\"PrepTime\"].quantile(0.75)\n",
    "    IQR_PrepTime = Q3_PrepTime -Q1_PrepTime\n",
    "    df_recipes = df_recipes[~((df_recipes[\"PrepTime\"]>(Q3_PrepTime+3*IQR_PrepTime)))]\n",
    "    \n",
    "    #3. Entfernen von falschen Content-Werten fuer Makronaehrstoffe\n",
    "    \n",
    "    max_Cal_per_100g = 900\n",
    "    max_Fat_per_100g = 900\n",
    "    max_Carbohydrates_per_100g = 420\n",
    "    max_Proteins_per_100g = 420\n",
    "    df_recipes = df_recipes[~((df_recipes[\"Calories\"]>max_Cal_per_100g))]\n",
    "    df_recipes = df_recipes[~((df_recipes[\"FatContent\"]>max_Fat_per_100g))]\n",
    "    df_recipes = df_recipes[~((df_recipes[\"CarbohydrateContent\"]>max_Carbohydrates_per_100g))]\n",
    "    df_recipes = df_recipes[~((df_recipes[\"SugarContent\"]>max_Carbohydrates_per_100g))]\n",
    "    df_recipes = df_recipes[~((df_recipes[\"ProteinContent\"]>max_Proteins_per_100g))]\n",
    "    \n",
    "    \n",
    "    #4. Entfernen von Outliers fuer Content-Werte von Mikronaehrstoffen\n",
    "    #Outlier Removal SaturatedFatContent\n",
    "    \n",
    "    Q1_SatFat = df_recipes[\"SaturatedFatContent\"].quantile(0.25)\n",
    "    Q3_SatFat = df_recipes[\"SaturatedFatContent\"].quantile(0.75)\n",
    "    IQR_SatFat = Q3_SatFat-Q1_SatFat\n",
    "    df_recipes = df_recipes[~((df_recipes[\"SaturatedFatContent\"]>((Q3_SatFat+3*IQR_SatFat))))]\n",
    "    #Outlier Removal CholesterolContent\n",
    "    Q1_Cholesterol = df_recipes[\"CholesterolContent\"].quantile(0.25)\n",
    "    Q3_Cholesterol = df_recipes[\"CholesterolContent\"].quantile(0.75)\n",
    "    IQR_Cholesterol = Q3_Cholesterol-Q1_Cholesterol\n",
    "    df_recipes = df_recipes[~((df_recipes[\"CholesterolContent\"]>((Q3_Cholesterol+3*IQR_Cholesterol))))]\n",
    "    #Outlier Removal SodiumContent\n",
    "    Q1_Sodium = df_recipes[\"SodiumContent\"].quantile(0.25)\n",
    "    Q3_Sodium = df_recipes[\"SodiumContent\"].quantile(0.75)\n",
    "    IQR_Sodium = Q3_Sodium-Q1_Sodium\n",
    "    df_recipes = df_recipes[~((df_recipes[\"SodiumContent\"]>((Q3_Sodium+3*IQR_Sodium))))]\n",
    "    #Outlier Removal Fiber\n",
    "    Q1_Fiber = df_recipes[\"FiberContent\"].quantile(0.25)\n",
    "    Q3_Fiber = df_recipes[\"FiberContent\"].quantile(0.75)\n",
    "    IQR_Fiber = Q3_Fiber-Q1_Fiber\n",
    "    df_recipes = df_recipes[~((df_recipes[\"FiberContent\"]>((Q3_Fiber+3*IQR_Fiber))))]\n",
    "    \n",
    "    #5. Hinzufuegen eines Rezepttypes -> Omnivore, Vegan oder Vegetarisch\n",
    "    erstelle_Meal_Typen()\n",
    "\n",
    "    #6. Fuellen von NA Werten in RecipeServings\n",
    "    df_recipes[\"RecipeServings\"] = df_recipes[\"RecipeServings\"].fillna(0)\n",
    "\n",
    "    #7. Anpassen der RecipeYield Spalte\n",
    "    df_recipes[\"RecipeYield\"] = df_recipes[\"RecipeYield\"].fillna(\"Undefined\")\n",
    "    df_recipes[\"RecipeYield\"] = df_recipes[\"RecipeYield\"].apply(translate_recipe_yields_to_categories)\n",
    "    df_recipes[\"RecipeYield\"] = df_recipes[\"RecipeYield\"].astype(\"category\")\n",
    "\n",
    "    #8. Entfernen von unnoetigen Spalten\n",
    "    df_recipes = df_recipes.drop(columns={\"Name\", \"RecipeIngredientQuantities\", \"RecipeIngredientParts\"})\n",
    "    \n",
    "    #9. Kategorisierung der letzten Attribute\n",
    "    df_recipes[\"RecipeCategory\"] = df_recipes[\"RecipeCategory\"].astype(\"category\")\n",
    "    df_recipes[\"Type_Meal\"] = df_recipes[\"Type_Meal\"].astype(\"category\")\n",
    "\n",
    "    #10. Save Recipe-Set\n",
    "    df_recipes_clean = df_recipes\n",
    "    df_recipes.to_csv(\"cleaned_recipes.csv\", index=False)\n",
    "    \n",
    "#-------------- Anpassen finales Dataset -------------------------#\n",
    "def modify_final_dataset():\n",
    "    global df_final\n",
    "    ##Encoding von Categorical Variables\n",
    "    df_final = df_final.drop(columns={\"AuthorId\", \"RecipeId\"})\n",
    "    non_numeric_cols_train = df_final.select_dtypes(include=['object', 'category']).columns\n",
    "    df_final = pd.get_dummies(df_final, columns=non_numeric_cols_train, drop_first=True)\n",
    "    #Likes nach hinten\n",
    "    likes = df_final[\"Like\"]\n",
    "    df_final = df_final.drop(columns= {\"Like\"})\n",
    "    df_final[\"Like\"] = likes\n",
    "\n",
    "#-------------- Mergen Datasets -------------------------#\n",
    "\n",
    "#Laden der Cleanen Datasets\n",
    "clean_diet()\n",
    "clean_reviews()\n",
    "clean_requests()\n",
    "clean_recipes()\n",
    "\n",
    "\n",
    "#Mergen der Datasets\n",
    "df_1 = pd.merge(df_diet_clean, df_trainset_clean, on = \"AuthorId\", how = \"inner\")\n",
    "df_2 = pd.merge(df_1, df_recipes_clean, on= \"RecipeId\", how=\"inner\")\n",
    "df_final = pd.merge(df_2, df_requests_clean, on =[\"AuthorId\", \"RecipeId\"], how= \"inner\")\n",
    "df_final.to_csv(\"cleaned_final.csv\", index=False)\n",
    "\n",
    "modify_final_dataset()\n",
    "#print(df_final)\n",
    "\n",
    "def build_model():\n",
    "    train_df, test_df = train_test_split(df_final, test_size=0.20, stratify=df_final['Like'], random_state=2023+2024)\n",
    "    X_train = train_df.drop(columns={\"Like\"})\n",
    "    y_train = train_df[\"Like\"]\n",
    "    X_test = test_df.drop(columns={\"Like\"})\n",
    "    y_test = test_df[\"Like\"]\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    # Modelle definieren\n",
    "    model_random_forest = RandomForestClassifier()\n",
    "    model_gradient_boosting = GradientBoostingClassifier()\n",
    "    model_logistic_regression = LogisticRegression(max_iter=30)\n",
    "    model_naive_bayes = GaussianNB()\n",
    "\n",
    "    # Pipeline ohne PCA\n",
    "    pipeline = Pipeline(steps=[(\"model\", model_random_forest)])\n",
    "    \n",
    "\n",
    "    # Parameter Grid nur für RandomForestClassifier\n",
    "    parameter_grid_random_forest = {\n",
    "    \"model__n_estimators\": [10, 20, 50],  # Anzahl der Bäume im Wald\n",
    "    \"model__max_depth\": [2, 3, 4], \n",
    "    \"model__max_features\": [3,5,20] # maximale Tiefe der Bäume\n",
    "    #\"model__n_estimators\": [50],  # Anzahl der Bäume im Wald\n",
    "    #\"model__max_depth\": [4], \n",
    "    #\"model__max_features\": [5] # maximale Tiefe der Bäume\n",
    "    }\n",
    "    # Grid Search Setup\n",
    "    search = GridSearchCV(pipeline,\n",
    "                      parameter_grid_random_forest, \n",
    "                      scoring=\"balanced_accuracy\",\n",
    "                      n_jobs=2, \n",
    "                      cv=5,  # Anzahl der Folds für die Kreuzvalidierung\n",
    "                      error_score=\"raise\"\n",
    "    )\n",
    "    # Training und Grid Search\n",
    "    search.fit(X_train_smote, y_train_smote)\n",
    "    # Beste Parameter ausgeben\n",
    "    \n",
    "    print(\"Beste Parameter:\", search.best_params_, \"(CV Score=%0.3f)\" % search.best_score_)\n",
    "    probabilities = search.predict_proba(X_test)\n",
    "    threshold= 0.15\n",
    "    predicted_classes = (probabilities[:, 1] >= threshold).astype(int)\n",
    "    print(balanced_accuracy_score(y_test, predicted_classes))\n",
    "    ct = pd.crosstab(predicted_classes, y_test,\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "    print(ct)\n",
    "\n",
    "    # evaluate performance of model on test set\n",
    "    print(\"Score on test set:\", search.score(X_test, y_test))\n",
    "    # contingency table\n",
    "    ct = pd.crosstab(search.best_estimator_.predict(X_test), y_test,\n",
    "                 rownames=[\"pred\"], colnames=[\"true\"])\n",
    "    print(ct)\n",
    "\n",
    "build_model()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "i2dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
